const Scrapely = require('./index');
const { quickScrape, DataUtils } = require('./index');

/**
 * Simple test to verify the library is working
 */

async function testBasicFunctionality() {
  console.log('ğŸ§ª Testing Scrapely Library...\n');
  
  try {
    // Test 1: Basic scraper creation
    console.log('âœ“ Test 1: Creating scraper instance...');
    const scraper = new Scrapely({
      timeout: 10000,
      maxRetries: 2
    });
    console.log('  âœ… Scraper instance created successfully\n');
    
    // Test 2: Load a page
    console.log('âœ“ Test 2: Loading example.com...');
    const $ = await scraper.load('https://example.com');
    console.log('  âœ… Page loaded successfully\n');
    
    // Test 3: Extract text
    console.log('âœ“ Test 3: Extracting title...');
    const title = await scraper.getText('https://example.com', 'h1');
    console.log(`  âœ… Title: "${title}"\n`);
    
    // Test 4: Extract multiple elements
    console.log('âœ“ Test 4: Extracting all paragraphs...');
    const paragraphs = await scraper.getText('https://example.com', 'p', { multiple: true });
    console.log(`  âœ… Found ${paragraphs.length} paragraphs\n`);
    
    // Test 5: Extract attributes
    console.log('âœ“ Test 5: Extracting link href...');
    const link = await scraper.getAttribute('https://example.com', 'a', 'href');
    console.log(`  âœ… Link: "${link}"\n`);
    
    // Test 6: Complex extraction
    console.log('âœ“ Test 6: Complex data extraction...');
    const data = await scraper.extract('https://example.com', {
      title: { selector: 'h1', type: 'text' },
      firstParagraph: { selector: 'p', type: 'text' },
      link: { selector: 'a', type: 'attribute', attribute: 'href' }
    });
    console.log('  âœ… Data extracted:', JSON.stringify(data, null, 2), '\n');
    
    // Test 7: Element existence check
    console.log('âœ“ Test 7: Checking element existence...');
    const hasH1 = await scraper.exists('https://example.com', 'h1');
    const hasFooter = await scraper.exists('https://example.com', 'footer');
    console.log(`  âœ… Has H1: ${hasH1}, Has Footer: ${hasFooter}\n`);
    
    // Test 8: Count elements
    console.log('âœ“ Test 8: Counting elements...');
    const linkCount = await scraper.count('https://example.com', 'a');
    const paragraphCount = await scraper.count('https://example.com', 'p');
    console.log(`  âœ… Links: ${linkCount}, Paragraphs: ${paragraphCount}\n`);
    
    // Test 9: Quick scrape utility
    console.log('âœ“ Test 9: Testing quick scrape utility...');
    const quickTitle = await quickScrape.getText('https://example.com', 'h1');
    console.log(`  âœ… Quick Title: "${quickTitle}"\n`);
    
    // Test 10: Custom headers
    console.log('âœ“ Test 10: Testing custom headers...');
    scraper.setHeaders({ 'Accept-Language': 'en-US' });
    console.log('  âœ… Custom headers set\n');
    
    // Test 11: Cookie setting
    console.log('âœ“ Test 11: Testing cookie setting...');
    scraper.setCookies({ test: 'value' });
    console.log('  âœ… Cookies set\n');
    
    // Test 12: Data utilities
    console.log('âœ“ Test 12: Testing data utilities...');
    const cleanedText = DataUtils.cleanText('  Extra   spaces  ');
    const price = DataUtils.parsePrice('$1,234.56');
    const domain = DataUtils.getDomain('https://example.com/path');
    console.log(`  âœ… Cleaned: "${cleanedText}", Price: ${price}, Domain: ${domain}\n`);
    
    // Test 13: Email extraction
    console.log('âœ“ Test 13: Testing email extraction...');
    const emails = await scraper.extractEmails('https://example.com');
    console.log(`  âœ… Emails found: ${emails.length}\n`);
    
    // Test 14: Link extraction
    console.log('âœ“ Test 14: Testing link extraction...');
    const links = await scraper.extractLinks('https://example.com', { unique: true });
    console.log(`  âœ… Links found: ${links.length}\n`);
    
    // Test 15: Cache functionality
    console.log('âœ“ Test 15: Testing cache...');
    const cachedScraper = new Scrapely({ cache: true });
    await cachedScraper.fetch('https://example.com');
    const cacheSize = cachedScraper.cacheSize;
    console.log(`  âœ… Cache size: ${cacheSize}\n`);
    
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
    console.log('ğŸ‰ All tests passed successfully!');
    console.log('â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n');
    
    console.log('ğŸ“Š Summary:');
    console.log(`   â€¢ Scraper creation: âœ…`);
    console.log(`   â€¢ Page loading: âœ…`);
    console.log(`   â€¢ Text extraction: âœ…`);
    console.log(`   â€¢ Multiple elements: âœ…`);
    console.log(`   â€¢ Attribute extraction: âœ…`);
    console.log(`   â€¢ Complex extraction: âœ…`);
    console.log(`   â€¢ Element existence: âœ…`);
    console.log(`   â€¢ Element counting: âœ…`);
    console.log(`   â€¢ Quick scrape: âœ…`);
    console.log(`   â€¢ Custom headers: âœ…`);
    console.log(`   â€¢ Cookie setting: âœ…`);
    console.log(`   â€¢ Data utilities: âœ…`);
    console.log(`   â€¢ Email extraction: âœ…`);
    console.log(`   â€¢ Link extraction: âœ…`);
    console.log(`   â€¢ Cache system: âœ…`);
    console.log('\nâœ¨ Scrapely is ready to use!');
    
  } catch (error) {
    console.error('âŒ Test failed:', error.message);
    console.error(error);
    process.exit(1);
  }
}

// Run tests
if (require.main === module) {
  testBasicFunctionality().catch(console.error);
}

module.exports = { testBasicFunctionality };
